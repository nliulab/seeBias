% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate_prediction.R
\name{evaluate_prediction_prob}
\alias{evaluate_prediction_prob}
\title{Evaluate algorithm fairness across patient subgroups based on predicted probabilities}
\usage{
evaluate_prediction_prob(
  y_pred,
  y_pred_threshold = NULL,
  y_obs,
  y_pos = "1",
  sens_var,
  sens_var_ref = NULL
)
}
\arguments{
\item{y_pred}{A numeric vector of predicted probabilities. Should not contain
missing value.}

\item{y_pred_threshold}{Threshold for \code{y_pred}. Predict positive label
if \code{y_pred > y_pred_threshold}. Default is \code{NULL}, in which case
the threshold will be selected based on the ROC curve for \code{y_pred}.}

\item{y_obs}{A vector of observed binary outcome. Can be numeric, character
or factor. Should not contain missing value.}

\item{y_pos}{A character representing the positive class in \code{y_obs}. If
\code{y_obs} is a factor, by default \code{y_pos = levels(y_obs)[2]}. If
\code{y_obs} is numeric or character, by default \code{y_pos = "1"} for 0/1
encoding.}

\item{sens_var}{Sensitive variable(s). Must be categorical.}

\item{sens_var_ref}{Reference class(es) of sensitive variable(s). Default is the
default reference category in \code{sens_var}.}
}
\value{
Returns a data.frame of performance metrics evaluated within each
  sensitive group.
}
\description{
Evaluate algorithm fairness across patient subgroups based on predicted probabilities
}
